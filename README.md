# Misogi PDF Q&A ChatBot

A full-stack AI-powered chatbot that lets you upload a PDF and ask questions about its content. The app extracts text from your PDF, splits it into chunks, stores them in a vector database (ChromaDB), and uses a local LLM (Ollama) to answer your questions based on semantic search.

---

## Features

- Upload any PDF (text-based)
- Asks questions about the uploaded PDF
- Uses local embeddings and LLM (no cloud required)
- Fast semantic search with ChromaDB
- Simple React frontend

---

## Tech Stack

- **Frontend:** React, Axios
- **Backend:** Node.js, Express, Multer
- **PDF Parsing:** pdf-parse
- **Embeddings:** @xenova/transformers (MiniLM)
- **Vector DB:** ChromaDB
- **LLM:** Ollama (Gemma 2B)
- **OCR (optional):** tesseract.js (for scanned PDFs)

---

## Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/yourusername/misogi-mac-challenge.git
cd misogi-mac-challenge
```

### 2. Install dependencies

#### Backend

```bash
cd server
npm install
```

#### Frontend

```bash
cd ../client
npm install
```

### 3. Start ChromaDB and Ollama

- **ChromaDB:**  
  Make sure ChromaDB is running locally (default: `http://localhost:8000`).

- **Ollama:**  
  Install [Ollama](https://ollama.com/) and pull the Gemma 2B model:
  ```bash
  ollama pull gemma:2b
  ollama serve
  ```

### 4. Start the servers

#### Backend

```bash
cd ../server
node server.js
```

#### Frontend

```bash
cd ../client
npm start
```

---

## Usage

1. Open [http://localhost:3000](http://localhost:3000) in your browser.
2. Upload a text-based PDF.
3. Ask any question about the PDF content.
4. View the answer generated by the local LLM.

---

## Project Structure

```
misogi-mac-challenge/
├── client/           # React frontend
│   └── src/
│       ├── App.js
│       └── components/
│           ├── UploadPDF.js
│           └── AskPDF.jsx
├── server/           # Node.js backend
│   ├── controller/
│   │   ├── pdfcontroller.js
│   │   └── querycontroller.js
│   ├── routes/
│   │   ├── pdf.js
│   │   └── query.js
│   └── utils/
│       ├── chunker.js
│       ├── ollama.js
│       └── vectorstore.js
```

---

## Notes

- For best results, use text-based PDFs. Scanned/image PDFs require OCR (not enabled by default).
- All processing (embedding, LLM) is local—no data leaves your machine.
- Large PDFs may take longer to process.

---

## Troubleshooting

- **Garbled answers?**  
  Your PDF may be scanned or not text-based. Try a different PDF or add OCR support.
- **Slow processing?**  
  Embedding and LLM are CPU-intensive. Use smaller PDFs or run on a machine with more resources.
- **ChromaDB or Ollama errors?**  
  Ensure both services are running before starting the backend.

---

## License

MIT

---

## Credits

- [Ollama](https://ollama.com/)
- [ChromaDB](https://www.trychroma.com/)
- [@xenova/transformers](https://github.com/xenova/transformers.js)
- [pdf-parse](https://www.npmjs.com/package/pdf-parse)

